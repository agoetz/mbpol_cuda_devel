{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a simple two_layer Neural Network\n",
    "\n",
    "This example is an exercise from open course CS231n Convolutional Neural Networks for Visual Recognition from Stanford university: http://cs231n.stanford.edu/\n",
    "\n",
    "The credit will be given to the authors of the course material.\n",
    "\n",
    "\n",
    "\n",
    "In this work we will develop a neural network with fully-connected layers to perform classification.\n",
    "\n",
    "The NN model consists of two layers, and a final classifier/loss caculation: \n",
    "1 A fully connected layer, with ReLN nonlinearity:\n",
    "\n",
    "Fully connected layer: output_vector = input_vector * weight_matrix + bias_vector;\n",
    "\n",
    "ReLN nonlinearity : output_vector = max(0, input_vector);\n",
    "        \n",
    "In mathmatical expression, the output vector $h_1$ of this layer is:\n",
    "       $$ h1 = max( 0, (x * W1 + b1) )$$ \n",
    "where $x$ is the input vector (a sample), $W1$ and $b1$ are weight matrix and bias vector respectively.\n",
    "    \n",
    "2 A fully connected layer.\n",
    "$$ h2 = h1 * W2 + b2$$\n",
    "\n",
    "\n",
    "3 The final loss of the output classifier (the weight that it predicts model INCORRECTLY) uses softmax classifier. The softmax classifier means, the element at index $i$ in output vector ($h_i$) equals its exponential probability in the output vector: \n",
    "$$ h3_i = \\frac{exp(h2_{i})} {\\sum\\limits_{j} exp(h2_j)} $$\n",
    "\n",
    "The final loss equals to the negative value of logarithm of $h$ at correct classifier index:\n",
    "For a sample $x$ whose correct classifier is $y$, its loss is:\n",
    "\n",
    " $$ L =  - log(h3_y) = -   log(    \\frac{exp(h2_{y})} {\\sum\\limits_{j} exp(h2_j)}      )           $$  \n",
    "\n",
    "\n",
    "As a example, if an input sample vector $x$ has its correct classifier $y=1$, and the output $h3$ classifier is a 5-element vector, then the loss on this sample is the negative value of logarithm of $h$ at index 1:\n",
    "$$ L =  - log(h3_y) = -log(h3_1) $$\n",
    "\n",
    "\n",
    "Further notice, if multiple samples are considered, the final loss is the average loss from each sample.\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neural_net import TwoLayerNet\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the class `TwoLayerNet` in the file `neural_net.py` to represent instances of our network. The network parameters are stored in the instance variable `self.params` where keys are string parameter names and values are numpy arrays. Below, we initialize toy data and a toy model that we will use to develop your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 16.24345364  -6.11756414  -5.28171752 -10.72968622]\n",
      " [  8.65407629 -23.01538697  17.44811764  -7.61206901]\n",
      " [  3.19039096  -2.49370375  14.62107937 -20.60140709]\n",
      " [ -3.22417204  -3.84054355  11.33769442 -10.99891267]\n",
      " [ -1.72428208  -8.77858418   0.42213747   5.82815214]]\n"
     ]
    }
   ],
   "source": [
    "# X is the input matrix.\n",
    "# each row is a sample\n",
    "# multiple rows means we test multiple samples simultaneously\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "# y is the correct classifier for each sample\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17640523  0.04001572  0.0978738   0.22408932  0.1867558  -0.09772779\n",
      "   0.09500884 -0.01513572 -0.01032189  0.04105985]\n",
      " [ 0.01440436  0.14542735  0.07610377  0.0121675   0.04438632  0.03336743\n",
      "   0.14940791 -0.02051583  0.03130677 -0.08540957]\n",
      " [-0.25529898  0.06536186  0.08644362 -0.0742165   0.22697546 -0.14543657\n",
      "   0.00457585 -0.01871839  0.15327792  0.14693588]\n",
      " [ 0.01549474  0.03781625 -0.08877857 -0.19807965 -0.03479121  0.0156349\n",
      "   0.12302907  0.12023798 -0.03873268 -0.03023028]]\n"
     ]
    }
   ],
   "source": [
    "print(net.params['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.   0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]\n"
     ]
    }
   ],
   "source": [
    "print(net.params['b1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1048553  -0.14200179 -0.17062702]\n",
      " [ 0.19507754 -0.05096522 -0.04380743]\n",
      " [-0.12527954  0.07774904 -0.16138978]\n",
      " [-0.02127403 -0.08954666  0.03869025]\n",
      " [-0.05108051 -0.11806322 -0.00281822]\n",
      " [ 0.04283319  0.00665172  0.03024719]\n",
      " [-0.06343221 -0.03627412 -0.06724604]\n",
      " [-0.03595532 -0.08131463 -0.17262826]\n",
      " [ 0.01774261 -0.04017809 -0.16301983]\n",
      " [ 0.04627823 -0.09072984  0.00519454]]\n"
     ]
    }
   ],
   "source": [
    "print(net.params['W2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.   0.2  0.4]\n"
     ]
    }
   ],
   "source": [
    "print(net.params['b2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass: compute scores\n",
    "Open the file `neural_net.py`, see the first part of the forward pass which uses the weights and biases to compute the scores for all inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "[[-0.82172636 -1.2186243  -0.328118  ]\n",
      " [-0.1673226  -1.16037191 -0.22064339]\n",
      " [-0.50381418 -0.9880024  -0.5997831 ]\n",
      " [-0.15021874 -0.45863519 -0.27655848]\n",
      " [ 0.03443199 -0.07557897  0.08156292]]\n"
     ]
    }
   ],
   "source": [
    "scores = net.loss(X)\n",
    "print('Your scores:')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass: compute loss\n",
    "In the same function, the second part computes the data and regularizaion loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.26619342637\n",
      "loss for each sample = [ 1.19713533  1.83397888  1.02208699  1.08795739  1.18980855]\n"
     ]
    }
   ],
   "source": [
    "loss, _ = net.loss(X, y, reg=0.05)\n",
    "print (\"loss =\", loss)\n",
    "print (\"loss for each sample =\", net.loss_eachsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward pass\n",
    "Compute the gradient of the loss with respect to the variables `W1`, `b1`, `W2`, and `b2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss, grads = net.loss(X, y, reg=0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "To train the network we will use stochastic gradient descent (SGD), similar to the SVM and Softmax classifiers. Look at the function `TwoLayerNet.train` and fill in the missing sections to implement the training procedure. This should be very similar to the training procedure you used for the SVM and Softmax classifiers. You will also have to implement `TwoLayerNet.predict`, as the training process periodically performs prediction to keep track of accuracy over time while the network trains.\n",
    "\n",
    "Once you have implemented the method, run the code below to train a two-layer network on toy data. You should achieve a training loss less than 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss:  0.0155149278736\n"
     ]
    }
   ],
   "source": [
    "net = init_toy_model()\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, reg=5e-6,\n",
    "            num_iters=100, verbose=False)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
